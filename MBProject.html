<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>PKIndustries</title>

    <link href="https://fonts.googleapis.com/css2?family=Azeret+Mono:wght@400;500;700&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="PKIndustries.css"/>

</head>

<body>
  
<ul>
  <li style="float:left"><a href="PKIndustries.html">PK Industries</a></li>
  <li style="border-right:none"><a href="aboutUs.html" >About Us</a></li>
  <li><a href="Projects.html"  class="active">Projects</a></li>
  <li><a href="#">Blog</a></li>
  <li><a href="#">My Space</a></li>
</ul>

<div class="rectangle"></div>

<h1>
  <span>Moonbeam Digital Synthesizer</span>
  <img src="images/Moonbeam Synth Promo.png">
</h1>
<div class="blogWords">
<h2>
    What is it?
</h2>

<p>
    Starting out as a MIDI controller and evolving into a standalone synth, the Moonbeam Digital Synthesizer aims to push the limits of microcontroller sound capabilities
</p>

<h2>
  Some backstory now
</h2>

<p>
  After browsing the Hack Club slack, (really go check them out they're great) I saw a new hardware program released,
  Blueprint. Similar to previous events this one provides funding for hardware projects of different levels of complexity. 
  After just finishing the design for the Cyber Watch32 (you can check that out here) I was burned out on brain power and 
  wanted to follow a guide for my next project. I had always wanted to do some kind of audio project and conveniently there was 
  a guide for a MIDI keyboard. After doing a lot of research though, I realized that MIDI controllers need an external software, 
  often called a Digital Audio Workstation or DAW to work. I imagined a more complete all in one station to create music, add effects
  and maybe even loop. What I wanted was a synthesizer
</p>

<h2>
  The plan
</h2>

<p>
  All good projects start with a good plan. I did a ton of research on lots of different ways to make sound. I had previously pulled a speaker 
  out of an old DVD player and I had a pack of buttons to act as piano keys for now. As always, however, software became my weakness.
  There are a couple of ways to make sound, but the general idea is that you send analog data, either PWM or true analog, out of an IO 
  and conect it to a speaker or amplifier. The faster the oscillations of that wave, the higher the pitch. This is where "A = 440Hz" comes from, 
  a wave going back and forth 440 times per second creates an "A" note. Sound is hard to get right, because having too slow of a loop causes pitches to shift
  and loads of other problems, so I ended up choosing the Raspberry Pi Pico microcontroller which should be plenty fast for this project.
  There are also a few audio libraries to abstract this away and make life a lot easier.
  I found a few online examples using the Mozzi audio library and even got sound to come through, albeit very faintly, but after hours of reasearch 
  I was unable to find a single project that made a standalone synthesizer using the Mozzi library. (Though there is a pretty good one using an Arduino and
  manually playing sin waves)
</p>

<h2>
  The prototype
</h2>

<p>
  Through a lot of sketchy arduino code I got different notes to play when one of 12 buttons were pressed. I then added 3 slide potentiometers, an 128px OLED 
  and rotary encoders and put all of it on a breadboard. One slider to control volume, one for pitch, and one for vibrator (the shake and color of a note) 
  There were a couple issues, mostly with playing notes at the same time, but it worked well enough to start designing a board and making this a full featured project. 
</p>

<h2>
  Board design
</h2>

<p>
  My favorite part of any project is designing a circuit board. I started with drawing out the schematic, which was pretty easy. This is where I normally 
  put all the ideas I have down and refine it later. The 25 piano keys are multiplexed keyboard switches only taking up 10 GPIOs, with the caveat of needing diodes 
  on every switch. To handle the audio side, 3 pins are connected to an I2S Digital to Analog Convertor. The PCM5100 chip is used to provide higher quality sound and hopefully 
  fix a couple audio issues. The sliders are connected to the 3 analog inputs and the 2 rotary encoders take up another 6 pins. With the I2C oled, only two GPIOs are unused, 
  with one connected to an LED and the other as a test pad for PWM audio if the I2S audio fails.  
</p>

<p>
  Moving this all onto the board was a little challenging due to the sheer size of components. The final board ended up beign about 350mm x 90mm in order to fit all the switches.
  The audio chips and passive components for those weren't too bad to route and I'm really happy with the final design.
</p>

<h2>
  The code
</h2>

<p>
  The code my be favorite part of this entire project. The Raspberry Pi Pico is a joy to program in VS Code using the PlatformIO extension, which turns it into 
  everything ArduinoIDE wishes it could be. The most challenging part was making use of both cores of the Pico. It's a very fast microcontroller, but with the speed that
  Mozzi requires, it was the only option in order to make use of the OLED. The first core scans the keyboard matrix, analog sliders, and event flags from the user input on the 
  second core. It uses this to mix sound, output it to the DAC, and record the last 128 samples for the second core to use. The other portion handles all the user input. 
  It scans the rotary encoders, and handles the menu for the OLED. 

  The OLED has 4 main tabs, with a couple more planned for the future. The main tab shows each of the 5 voices and what note they are playing. It also shows the states of the 
  analog sliders and was very helpful for debugging. The waveform selector tab was really simple to implement, but due to both cores running at the same time, some care had to be taken 
  with the variable changing. The waveform selector (and most other thigns that affect audio) set a shared atomic boolean flag, which the audio core than handles the switching of 
  the waveform and effects. The ADSR selector was also really fun to implement. Using the rotary encoders you can set the times and levels of the Attack, Decay, Sustain, and Release 
  of the note. The code then scales these values to fit the screen and draws lines to connect them showing an accurate graph. Finally the waveform viewer is the most complicated part of 
  the entire codebase. Using the 128 samples received from the audio core, it draws lines connecting them and scales accordingly. It shows different patterns based on the selected 
  waveform and number of notes playing, but because the buffer is circular (once it overflows it replaces the first item) the graph slides left and right and is hard to view. This is number 
  one on the bug list and should be solved soon.
</p>



</div>

 <div class="footer">
  <h2>Want to learn more?</h2>
  <p> Check out this projects <a href="https://github.com/Kacman62" target="_blank"  style="text-decoration:none">Github</a></p>

  <script src="PKIndustries.js"></script>
</body>
</html>
